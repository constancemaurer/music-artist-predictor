{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* KNN\n",
    "* Logistic Regression\n",
    "* Decision Tree Classifier\n",
    "* Random Forest Classifier\n",
    "* Bagging Classifier with KNN & Decision Tree Classifier\n",
    "* AdeBoost with Decision Tree Classifier\n",
    "* Multi-layer Perception Classifier\n",
    "* Support Vector Classifier (rbf, poly, sigmoid)\n",
    "* Linear Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Functions</a></span></li><li><span><a href=\"#Model-Predictors-and-Target-Prep\" data-toc-modified-id=\"Model-Predictors-and-Target-Prep-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model Predictors and Target Prep</a></span></li><li><span><a href=\"#Train-Test-Split\" data-toc-modified-id=\"Train-Test-Split-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Train-Test-Split</a></span></li><li><span><a href=\"#Dummyfication-of-Categorical-variables\" data-toc-modified-id=\"Dummyfication-of-Categorical-variables-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Dummyfication of Categorical variables</a></span></li><li><span><a href=\"#CountVectorizer-Model-Testing\" data-toc-modified-id=\"CountVectorizer-Model-Testing-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>CountVectorizer Model Testing</a></span></li><li><span><a href=\"#TfidfVectorizer-Model-Testing\" data-toc-modified-id=\"TfidfVectorizer-Model-Testing-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>TfidfVectorizer Model Testing</a></span></li><li><span><a href=\"#Large-Scale-Model-Testing-with-TfidfVectorizer-and-Engineered-Features\" data-toc-modified-id=\"Large-Scale-Model-Testing-with-TfidfVectorizer-and-Engineered-Features-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Large Scale Model Testing with TfidfVectorizer and Engineered Features</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "import scikitplot as skplt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling with sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "from sklearn import cluster\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StandardScaler_processing(X_train, X_test):\n",
    "    '''\n",
    "    This function takes train and test X variables and fit and transforms the StandardScaler() on the\n",
    "    X_train and transform the X_test. \n",
    "    It returns the standarised X_train and X_test dataframes.\n",
    "    '''\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummyifying(X, columns):\n",
    "    '''\n",
    "    This function accept the predictor variables and the specified columns that should be dummified and\n",
    "    returns the dataframe only containing the specified dummified features.\n",
    "    '''\n",
    "    X = pd.get_dummies(X, columns=columns, drop_first=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvec_processing(processing_column, X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    stop = stopwords.words('english')\n",
    "    stop += ['oh', 'ah', 've','ll','ooh','oooh','uh','aah','aaah','yeah','bum','na','la','doo','nah', 'eh','pow',\n",
    "             'di','oo','whoa','naa','em','ga','da','hi','sha','ba','wee']\n",
    "\n",
    "    cvec = CountVectorizer(strip_accents='unicode',\n",
    "                       stop_words=stop, \n",
    "                       ngram_range=(1, 3),\n",
    "                      min_df=0.01,\n",
    "                      max_features=1000)\n",
    "\n",
    "    train_matrix_c = cvec.fit_transform(X_train[processing_column])\n",
    "    \n",
    "    #remove the lyrics_processed column\n",
    "    #X_train.drop('lyrics_processed', axis=1, inplace=True)\n",
    "    \n",
    "    CVEC_train = pd.DataFrame(train_matrix_c.toarray(),\n",
    "                  columns=cvec.get_feature_names())\n",
    "    \n",
    "    #resetting the X_train index so it can be joined with the CVEC_train\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #resetting the y_train index \n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #joining the dataframe and the cvec_train dataframe\n",
    "    X_train = pd.concat([X_train, CVEC_train], axis=1, sort=False)\n",
    "    \n",
    "    \n",
    "    test_matrix_c = cvec.transform(X_test[processing_column])\n",
    "    CVEC_test = pd.DataFrame(test_matrix_c.toarray(),\n",
    "                  columns=cvec.get_feature_names())\n",
    "    \n",
    "    #remove the lyrics_processed column\n",
    "    #X_test.drop('lyrics_processed', axis=1, inplace=True)\n",
    "    \n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    X_test = pd.concat([X_test, CVEC_test], axis=1, sort=False)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tvec_processing(processing_column, X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    stop = stopwords.words('english')\n",
    "    stop += ['oh', 'ah', 've','ll','ooh','oooh','uh','aah','aaah','yeah','bum','na','la','doo','nah', 'eh','pow',\n",
    "             'di','oo','whoa','naa','em','ga','da','hi','sha','ba','wee']\n",
    "\n",
    "    tvec = TfidfVectorizer(strip_accents='unicode',\n",
    "                       stop_words=stop, \n",
    "                       ngram_range=(1, 3),\n",
    "                      max_features=1000,\n",
    "                      max_df = 0.9,\n",
    "                      min_df = 0.01,\n",
    "                      sublinear_tf=True)\n",
    "\n",
    "    train_matrix_t = tvec.fit_transform(X_train[processing_column])\n",
    "\n",
    "    \n",
    "    #remove the lyrics_processed column\n",
    "    #X_train.drop('lyrics_processed', axis=1, inplace=True)\n",
    "    \n",
    "    TVEC_train = pd.DataFrame(train_matrix_t.toarray(),\n",
    "                  columns=tvec.get_feature_names())\n",
    "    \n",
    "    #resetting the X_train index so it can be joined with the CVEC_train\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #resetting the y_train index \n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #joining the dataframe and the cvec_train dataframe\n",
    "    X_train = pd.concat([X_train, TVEC_train], axis=1, sort=False)\n",
    "    \n",
    "    \n",
    "    test_matrix_t = tvec.transform(X_test[processing_column])\n",
    "    TVEC_test = pd.DataFrame(test_matrix_t.toarray(),\n",
    "                  columns=tvec.get_feature_names())\n",
    "    \n",
    "    #remove the lyrics_processed column\n",
    "    #X_test.drop('lyrics_processed', axis=1, inplace=True)\n",
    "    \n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    X_test = pd.concat([X_test, TVEC_test], axis=1, sort=False)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_table(name,\n",
    "                model,\n",
    "                processing,\n",
    "                X_train,\n",
    "                y_train,\n",
    "                X_test,\n",
    "                y_test):\n",
    "    '''\n",
    "    The model function can accept a given model name, the model object, the X_train, y_train, X_test and y_test\n",
    "    as arguments to return a dataframe with the supplied model name, parameters, train and test set's accuracy, \n",
    "    precision, recall and F1.\n",
    "    '''\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    model_name = []\n",
    "    parameters = []\n",
    "    train_accuracy_scores = []\n",
    "    test_accuracy_scores = []\n",
    "    train_precisions = []\n",
    "    test_precisions = []\n",
    "    train_recalls = []\n",
    "    test_recalls = []\n",
    "    train_f1_scores = []\n",
    "    test_f1_scores = []\n",
    "    cross_val_scores = []\n",
    "\n",
    "    model_name.append(name)\n",
    "    parameters.append(model)\n",
    "    train_accuracy_scores.append('{0:.3f}'.format(model.score(\n",
    "        X_train, y_train)))\n",
    "    test_accuracy_scores.append('{0:.3f}'.format(model.score(X_test, y_test)))\n",
    "    train_precisions.append('{0:.3f}'.format(\n",
    "        precision_score(y_train, y_pred_train, average='macro')))\n",
    "    test_precisions.append('{0:.3f}'.format(\n",
    "        precision_score(y_test, y_pred_test, average='macro')))\n",
    "    train_recalls.append('{0:.3f}'.format(\n",
    "        recall_score(y_train, y_pred_train, average='macro')))\n",
    "    test_recalls.append('{0:.3f}'.format(\n",
    "        recall_score(y_test, y_pred_test, average='macro')))\n",
    "    train_f1_scores.append('{0:.3f}'.format(\n",
    "        f1_score(y_train, y_pred_train, average='macro')))\n",
    "    test_f1_scores.append('{0:.3f}'.format(\n",
    "        f1_score(y_test, y_pred_test, average='macro')))\n",
    "    cross_val_scores.append('{0:.3f}'.format(cross_val_score(model, X_train, y_train, cv=5).mean()))\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Model': model_name,\n",
    "        'Parameters': parameters,\n",
    "        'Processing': processing,\n",
    "        'Train: Accuracy': train_accuracy_scores,\n",
    "        'Train: Precision': train_precisions,\n",
    "        'Train: Recall': train_recalls,\n",
    "        'Train: F1': train_f1_scores,\n",
    "        'Test: Accuracy': test_accuracy_scores,\n",
    "        'Test: Precision': test_precisions,\n",
    "        'Test: Recall': test_recalls,\n",
    "        'Test: F1': test_f1_scores,\n",
    "        'Cross-Val Score': cross_val_scores\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_models(models, pars, model_names,processings, scoring, X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    This function is able to carry out multiple gridsearches in a row using the scikit-learn\n",
    "    GridSearchCV() class. \n",
    "    It accepts a list of models (models) from scikit-learn, a list of corresponding parameters (pars)\n",
    "    and a list of corresponding model names(model_names) and a list of previous processing steps(processings) \n",
    "    predefined by the user.\n",
    "    Furthermore, it accepts train and test set objects and the preferred scoring for then \n",
    "    gridsearch. \n",
    "    It only returns a table of all models run an it's results using the \n",
    "    model_table () function which uses scikit-learns classes.\n",
    "    Results include:\n",
    "        - model name\n",
    "        - parameters\n",
    "        - accuracies,\n",
    "        - precisions\n",
    "        - recalls\n",
    "        - F1 scores\n",
    "        - cross-val score of the best model \n",
    "        \n",
    "    Parameters:\n",
    "    ------------------------------\n",
    "    models: List of models predefined by the user.\n",
    "    pars: List of parameters predefined by the user.\n",
    "    model_names: List of strings of how the models are displayed in the table.\n",
    "    processings: List of strings describing each models previous processing steps.\n",
    "    scoring: None,'accuracy', 'precision', 'recall', etc please refer to scikit-learn documentation.\n",
    "    '''\n",
    "    \n",
    "    temp_df = pd.DataFrame({\n",
    "        'Model': [],\n",
    "        'Parameters': [],\n",
    "        'Processing': [],\n",
    "        'Train: Accuracy': [],\n",
    "        'Train: Precision': [],\n",
    "        'Train: Recall': [],\n",
    "        'Train: F1': [],\n",
    "        'Test: Accuracy': [],\n",
    "        'Test: Precision': [],\n",
    "        'Test: Recall': [],\n",
    "        'Test: F1': [],\n",
    "        'Cross-Val Score': []\n",
    "    })\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"starting Gridsearch\")\n",
    "    for i in range(len(models)):\n",
    "        print('Running Model {} / {}.'.format(i + 1, len(models)))\n",
    "        gs = GridSearchCV(models[i],\n",
    "                          pars[i],\n",
    "                          verbose=2,\n",
    "                          refit=True,\n",
    "                          n_jobs=-1,\n",
    "                          iid=False,\n",
    "                        scoring=scoring)\n",
    "        \n",
    "        gs_fit = gs.fit(X_train, y_train)\n",
    "\n",
    "        temp2_df = model_table(name=model_names[i],\n",
    "                               model=gs_fit.best_estimator_,\n",
    "                               processing = processings[i],\n",
    "                               X_train=X_train,\n",
    "                               y_train=y_train,\n",
    "                               X_test=X_test,\n",
    "                               y_test=y_test)\n",
    "        temp_df = pd.concat([temp_df, temp2_df])\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Predictors and Target Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removed artists which have less than 10 songs\n",
    "* Removed songs with only 'Instrumental'/no lyrics\n",
    "* Defined X and y variables\n",
    "* Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/constancemaurer/GA DSI 12/DSI12-lessons/projects/project-capstone/personal-github/Resources/capstone_feature_engineered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1336, 38)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.artist_name.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removed songs with only 'Instrumental'/no lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[data.lyrics_processed.isnull()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removed artists which have less than 10 songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The Black Keys           76\n",
       "Taylor Swift             58\n",
       "Mac Miller               58\n",
       "Lady Gaga                54\n",
       "Red Hot Chili Peppers    52\n",
       "Rihanna                  48\n",
       "Beyoncé                  48\n",
       "Slipknot                 47\n",
       "The Weeknd               47\n",
       "System Of A Down         45\n",
       "Kings of Leon            43\n",
       "Linkin Park              43\n",
       "Beach House              43\n",
       "Radiohead                41\n",
       "Lil Wayne                40\n",
       "John Legend              40\n",
       "Foo Fighters             38\n",
       "Arctic Monkeys           36\n",
       "Gorillaz                 36\n",
       "Tyler, The Creator       33\n",
       "A$AP Rocky               33\n",
       "Kendrick Lamar           30\n",
       "Frank Ocean              29\n",
       "Wiz Khalifa              29\n",
       "Fall Out Boy             26\n",
       "Korn                     24\n",
       "Aretha Franklin          23\n",
       "Adele                    23\n",
       "LCD Soundsystem          22\n",
       "Nirvana                  21\n",
       "Amy Winehouse            19\n",
       "Tame Impala              17\n",
       "Passion Pit              16\n",
       "Metallica                16\n",
       "Led Zeppelin             15\n",
       "Fleetwood Mac            12\n",
       "Caribou                  11\n",
       "The Rolling Stones       10\n",
       "HAIM                     10\n",
       "Queen                     6\n",
       "Sister Sledge             4\n",
       "The Beatles               3\n",
       "Twin Shadow               2\n",
       "Name: artist_name, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.artist_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 1135,\n",
       " 1136,\n",
       " 1137,\n",
       " 1272,\n",
       " 1273]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data[\n",
    "    (data.artist_name=='Queen')|\n",
    "    (data.artist_name=='Sister Sledge')|\n",
    "    (data.artist_name=='The Beatles')|\n",
    "    (data.artist_name=='Twin Shadow')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(index=list(data[(data.artist_name == 'Queen') | \n",
    "                          (data.artist_name == 'Sister Sledge')| \n",
    "                          (data.artist_name == 'The Beatles') |\n",
    "                          (data.artist_name == 'Twin Shadow')].index),\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1312, 38)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.artist_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Model_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining X and y variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excluded from X:\n",
    "- 'track_name'\n",
    "- 'artist_name'\n",
    "- 'release_year'\n",
    "- 'spotify_uri'\n",
    "- 'lyrics'\n",
    "- 'genre'\n",
    "- 'track_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['track_name', 'artist_name', 'release_year', 'spotify_uri', 'lyrics',\n",
       "       'genre', 'track_id', 'popularity', 'acousticness', 'danceability',\n",
       "       'duration_ms', 'energy', 'instrumentalness', 'key', 'liveness',\n",
       "       'loudness', 'mode', 'speechiness', 'tempo', 'time_signature', 'valence',\n",
       "       'n_sentences', 'word_count', 'character_count', 'n_syllables',\n",
       "       'unique_word_count', 'n_long_words', 'n_monosyllable_words',\n",
       "       'n_polysyllable_words', 'lyrics_processed', 'vader_compound',\n",
       "       'vader_neg', 'vader_neu', 'vader_pos', 'objectivity_score',\n",
       "       'pos_vs_neg', 'TTR', 'MTLD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['popularity',\n",
    "       'acousticness', 'danceability', 'duration_ms', 'energy',\n",
    "       'instrumentalness', 'key', 'liveness', 'loudness', 'mode',\n",
    "       'speechiness', 'tempo', 'time_signature', 'valence', 'n_sentences',\n",
    "       'word_count', 'character_count', 'n_syllables', 'unique_word_count',\n",
    "       'n_long_words', 'n_monosyllable_words', 'n_polysyllable_words','vader_compound', 'vader_neg', 'vader_neu',\n",
    "       'vader_pos', 'objectivity_score', 'pos_vs_neg', 'TTR', 'MTLD','lyrics_processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.artist_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.057926829268292686"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export train and test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.concat([X_train, y_train], axis=1, sort=False)\n",
    "train_set.reset_index(drop=True, inplace=True)\n",
    "train_set.to_csv('train_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.concat([X_test, y_test], axis=1, sort=False)\n",
    "test_set.reset_index(drop=True, inplace=True)\n",
    "test_set.to_csv('test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummyfication of Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['popularity', 'acousticness', 'danceability', 'duration_ms', 'energy',\n",
       "       'instrumentalness', 'key', 'liveness', 'loudness', 'mode',\n",
       "       'speechiness', 'tempo', 'time_signature', 'valence', 'n_sentences',\n",
       "       'word_count', 'character_count', 'n_syllables', 'unique_word_count',\n",
       "       'n_long_words', 'n_monosyllable_words', 'n_polysyllable_words',\n",
       "       'vader_compound', 'vader_neg', 'vader_neu', 'vader_pos',\n",
       "       'objectivity_score', 'pos_vs_neg', 'TTR', 'MTLD', 'lyrics_processed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "popularity                int64\n",
       "acousticness            float64\n",
       "danceability            float64\n",
       "duration_ms               int64\n",
       "energy                  float64\n",
       "instrumentalness        float64\n",
       "key                      object\n",
       "liveness                float64\n",
       "loudness                float64\n",
       "mode                     object\n",
       "speechiness             float64\n",
       "tempo                   float64\n",
       "time_signature           object\n",
       "valence                 float64\n",
       "n_sentences               int64\n",
       "word_count                int64\n",
       "character_count           int64\n",
       "n_syllables               int64\n",
       "unique_word_count         int64\n",
       "n_long_words              int64\n",
       "n_monosyllable_words      int64\n",
       "n_polysyllable_words      int64\n",
       "vader_compound          float64\n",
       "vader_neg               float64\n",
       "vader_neu               float64\n",
       "vader_pos               float64\n",
       "objectivity_score       float64\n",
       "pos_vs_neg              float64\n",
       "TTR                     float64\n",
       "MTLD                    float64\n",
       "lyrics_processed         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dummyifying(X_train, ['key', 'time_signature','mode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>...</th>\n",
       "      <th>key_D#</th>\n",
       "      <th>key_E</th>\n",
       "      <th>key_F</th>\n",
       "      <th>key_F#</th>\n",
       "      <th>key_G</th>\n",
       "      <th>key_G#</th>\n",
       "      <th>time_signature_3/4</th>\n",
       "      <th>time_signature_4/4</th>\n",
       "      <th>time_signature_5/4</th>\n",
       "      <th>mode_Minor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1139</td>\n",
       "      <td>44</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.374</td>\n",
       "      <td>184720</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.456000</td>\n",
       "      <td>0.2620</td>\n",
       "      <td>-2.413</td>\n",
       "      <td>0.0852</td>\n",
       "      <td>115.657</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>61</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.327</td>\n",
       "      <td>219800</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>-7.428</td>\n",
       "      <td>0.0367</td>\n",
       "      <td>169.390</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>746</td>\n",
       "      <td>53</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.715</td>\n",
       "      <td>260974</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3610</td>\n",
       "      <td>-5.426</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>119.994</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>65</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.231</td>\n",
       "      <td>255960</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.2900</td>\n",
       "      <td>-5.131</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>138.311</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>72</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.545</td>\n",
       "      <td>253747</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.0894</td>\n",
       "      <td>-4.062</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>99.099</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      popularity  acousticness  danceability  duration_ms  energy  \\\n",
       "1139          44      0.069500         0.374       184720   0.925   \n",
       "615           61      0.000564         0.327       219800   0.895   \n",
       "746           53      0.132000         0.715       260974   0.794   \n",
       "296           65      0.000036         0.231       255960   0.866   \n",
       "180           72      0.143000         0.545       253747   0.649   \n",
       "\n",
       "      instrumentalness  liveness  loudness  speechiness    tempo  ...  key_D#  \\\n",
       "1139          0.456000    0.2620    -2.413       0.0852  115.657  ...       0   \n",
       "615           0.015900    0.1040    -7.428       0.0367  169.390  ...       0   \n",
       "746           0.000000    0.3610    -5.426       0.1630  119.994  ...       0   \n",
       "296           0.000552    0.2900    -5.131       0.0517  138.311  ...       0   \n",
       "180           0.000016    0.0894    -4.062       0.0324   99.099  ...       0   \n",
       "\n",
       "      key_E  key_F  key_F#  key_G  key_G#  time_signature_3/4  \\\n",
       "1139      1      0       0      0       0                   0   \n",
       "615       0      0       0      0       0                   0   \n",
       "746       0      0       0      0       0                   0   \n",
       "296       0      0       0      0       0                   0   \n",
       "180       0      0       1      0       0                   0   \n",
       "\n",
       "      time_signature_4/4  time_signature_5/4  mode_Minor  \n",
       "1139                   1                   0           1  \n",
       "615                    1                   0           0  \n",
       "746                    1                   0           0  \n",
       "296                    1                   0           0  \n",
       "180                    1                   0           0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = dummyifying(X_test, ['key', 'time_signature','mode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>...</th>\n",
       "      <th>key_D#</th>\n",
       "      <th>key_E</th>\n",
       "      <th>key_F</th>\n",
       "      <th>key_F#</th>\n",
       "      <th>key_G</th>\n",
       "      <th>key_G#</th>\n",
       "      <th>time_signature_3/4</th>\n",
       "      <th>time_signature_4/4</th>\n",
       "      <th>time_signature_5/4</th>\n",
       "      <th>mode_Minor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>53</td>\n",
       "      <td>0.006870</td>\n",
       "      <td>0.372</td>\n",
       "      <td>209720</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-3.338</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>178.829</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>79</td>\n",
       "      <td>0.006160</td>\n",
       "      <td>0.288</td>\n",
       "      <td>201726</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.303</td>\n",
       "      <td>-5.692</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>97.094</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>36</td>\n",
       "      <td>0.847000</td>\n",
       "      <td>0.465</td>\n",
       "      <td>131200</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-10.847</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>93.981</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1324</td>\n",
       "      <td>47</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>0.694</td>\n",
       "      <td>259717</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-8.185</td>\n",
       "      <td>0.2780</td>\n",
       "      <td>135.966</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>654</td>\n",
       "      <td>71</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.365</td>\n",
       "      <td>248587</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.318</td>\n",
       "      <td>-5.429</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>79.119</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      popularity  acousticness  danceability  duration_ms  energy  \\\n",
       "262           53      0.006870         0.372       209720   0.898   \n",
       "84            79      0.006160         0.288       201726   0.758   \n",
       "117           36      0.847000         0.465       131200   0.431   \n",
       "1324          47      0.108000         0.694       259717   0.626   \n",
       "654           71      0.000707         0.365       248587   0.751   \n",
       "\n",
       "      instrumentalness  liveness  loudness  speechiness    tempo  ...  key_D#  \\\n",
       "262           0.000006     0.316    -3.338       0.1180  178.829  ...       0   \n",
       "84            0.000000     0.303    -5.692       0.0371   97.094  ...       0   \n",
       "117           0.000000     0.360   -10.847       0.0304   93.981  ...       0   \n",
       "1324          0.000000     0.343    -8.185       0.2780  135.966  ...       0   \n",
       "654           0.000000     0.318    -5.429       0.3040   79.119  ...       0   \n",
       "\n",
       "      key_E  key_F  key_F#  key_G  key_G#  time_signature_3/4  \\\n",
       "262       0      0       0      0       0                   0   \n",
       "84        0      0       1      0       0                   0   \n",
       "117       0      0       0      1       0                   0   \n",
       "1324      0      0       0      0       0                   0   \n",
       "654       0      0       0      0       0                   0   \n",
       "\n",
       "      time_signature_4/4  time_signature_5/4  mode_Minor  \n",
       "262                    1                   0           0  \n",
       "84                     1                   0           1  \n",
       "117                    1                   0           0  \n",
       "1324                   1                   0           0  \n",
       "654                    1                   0           0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts a collection of text documents to a matrix of token counts\n",
    "This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "- only requires a few text cleaning steps\n",
    "- simple way to both tokenize a collection of text documents and build a corpus\n",
    "- stop word removal is built into the class and can easily be extended with NLTK\n",
    "\n",
    "**Cons**\n",
    "\n",
    "- based on BOW (bag of words) and does not capture the position in semantics, co-occurrences in the document\n",
    "- suitable for a small corpus\n",
    "- vectors can become extremely sparse, particularly as vocabularies get larger, which can have a significant impact on the speed and performance of machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_c, y_train_c, X_test_c, y_test_c = cvec_processing('lyrics_processed', X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_c.drop('lyrics_processed', axis=1, inplace=True)\n",
    "X_test_c.drop('lyrics_processed', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_c, X_test_c = StandardScaler_processing(X_train_c, X_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = KNeighborsClassifier()\n",
    "\n",
    "model2 = LogisticRegression()\n",
    "\n",
    "model3 = RandomForestClassifier(random_state=42)\n",
    "\n",
    "model4 = SVC()\n",
    "\n",
    "\n",
    "parameters1 = {\n",
    "    'n_neighbors': range(1, 51),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "parameters2 = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['saga'],\n",
    "    'multi_class': ['auto'],\n",
    "    'C': np.logspace(-1, 1, 40),\n",
    "}\n",
    "\n",
    "parameters3 = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'n_estimators': [500],\n",
    "    'max_depth': [40, 50, 60, 70] + [None],\n",
    "    'max_features':\n",
    "    [None, 10, 50, 100, 500, 1000, X_train.shape[1]]\n",
    "}\n",
    "\n",
    "parameters4 = {\n",
    "    'C': [0.01, 0.1, 1.0],\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "    'gamma': [0.01, 0.1, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['KNN', \n",
    "               'Logistic Regression', \n",
    "               'Random Forest', \n",
    "               'Support Vector Machine']\n",
    "\n",
    "models = [model1, \n",
    "          model2, \n",
    "          model3, \n",
    "          model4]\n",
    "\n",
    "pars = [parameters1, \n",
    "        parameters2, \n",
    "        parameters3, \n",
    "        parameters4]\n",
    "\n",
    "processings = ['StandardScaler(), CountVectorizer()',\n",
    "               'StandardScaler(), CountVectorizer()',\n",
    "               'StandardScaler(), CountVectorizer()',\n",
    "               'StandardScaler(), CountVectorizer()']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Gridsearch\n",
      "Running Model 1 / 4.\n",
      "Fitting 3 folds for each of 200 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 2 / 4.\n",
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 16.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 3 / 4.\n",
      "Fitting 3 folds for each of 70 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   38.6s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed: 10.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 4 / 4.\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   11.5s finished\n"
     ]
    }
   ],
   "source": [
    "cvec_df = grid_search_models(models, \n",
    "                             pars, \n",
    "                             model_names, \n",
    "                             processings,\n",
    "                             'accuracy', \n",
    "                             X_train_c, \n",
    "                             y_train_c, \n",
    "                             X_test_c,\n",
    "                             y_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Processing</th>\n",
       "      <th>Train: Accuracy</th>\n",
       "      <th>Train: Precision</th>\n",
       "      <th>Train: Recall</th>\n",
       "      <th>Train: F1</th>\n",
       "      <th>Test: Accuracy</th>\n",
       "      <th>Test: Precision</th>\n",
       "      <th>Test: Recall</th>\n",
       "      <th>Test: F1</th>\n",
       "      <th>Cross-Val Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>KNN</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>StandardScaler(), CountVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>LogisticRegression(C=7.896522868499725, class_...</td>\n",
       "      <td>StandardScaler(), CountVectorizer()</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), CountVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>StandardScaler(), CountVectorizer()</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model                                         Parameters  \\\n",
       "0                     KNN  KNeighborsClassifier(algorithm='auto', leaf_si...   \n",
       "0     Logistic Regression  LogisticRegression(C=7.896522868499725, class_...   \n",
       "0           Random Forest  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "0  Support Vector Machine  SVC(C=1.0, cache_size=200, class_weight=None, ...   \n",
       "\n",
       "                            Processing Train: Accuracy Train: Precision  \\\n",
       "0  StandardScaler(), CountVectorizer()           1.000            1.000   \n",
       "0  StandardScaler(), CountVectorizer()           0.996            0.998   \n",
       "0  StandardScaler(), CountVectorizer()           1.000            1.000   \n",
       "0  StandardScaler(), CountVectorizer()           0.994            0.998   \n",
       "\n",
       "  Train: Recall Train: F1 Test: Accuracy Test: Precision Test: Recall  \\\n",
       "0         1.000     1.000          0.080           0.113        0.074   \n",
       "0         0.994     0.996          0.304           0.239        0.232   \n",
       "0         1.000     1.000          0.452           0.445        0.360   \n",
       "0         0.991     0.994          0.106           0.009        0.055   \n",
       "\n",
       "  Test: F1 Cross-Val Score  \n",
       "0    0.057           0.066  \n",
       "0    0.223           0.302  \n",
       "0    0.361           0.433  \n",
       "0    0.015           0.090  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_df.to_csv('CountVectorizer_model_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF–IDF is computed by the scaled frequency of the appearance of the term in the document, normalized by the inverse of the scaled frequency of the term in the entire corpus. This gives less frequent words a higher weight compared to common words.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "- only requires a few text cleaning steps\n",
    "- simple way to both tokenize a collection of text documents and build a corpus giving higher weights to rare words\n",
    "- stop word removal is built into the class and can easily be extended with NLTK\n",
    "- returns a sparse matrix representation in the form of ((doc, term), tfidf) which can easily be converted to dataframe\n",
    "\n",
    "**Cons**\n",
    "\n",
    "- based on BOW (bag of words) and does not capture the position in semantics, co-occurrences in the document\n",
    "- suitable for small corpus\n",
    "- vectors can become extremely sparse, particularly as vocabularies get larger, which can have a significant impact on the speed and performance of machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t, y_train_t, X_test_t, y_test_t = tvec_processing('lyrics_processed', X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t.drop('lyrics_processed', axis=1, inplace=True)\n",
    "X_test_t.drop('lyrics_processed', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t, X_test_t = StandardScaler_processing(X_train_t, X_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['KNN', \n",
    "               'Logistic Regression', \n",
    "               'Random Forest', \n",
    "               'Support Vector Machine']\n",
    "\n",
    "models = [model1, \n",
    "          model2, \n",
    "          model3, \n",
    "          model4]\n",
    "\n",
    "pars = [parameters1, \n",
    "        parameters2, \n",
    "        parameters3, \n",
    "        parameters4]\n",
    "\n",
    "processings = ['StandardScaler(), TfidfVectorizer()',\n",
    "               'StandardScaler(), TfidfVectorizer()',\n",
    "               'StandardScaler(), TfidfVectorizer()',\n",
    "               'StandardScaler(), TfidfVectorizer()']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Gridsearch\n",
      "Running Model 1 / 4.\n",
      "Fitting 3 folds for each of 200 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:   37.4s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 2 / 4.\n",
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 16.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 3 / 4.\n",
      "Fitting 3 folds for each of 70 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   45.4s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  7.9min\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed: 13.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 4 / 4.\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   13.4s finished\n"
     ]
    }
   ],
   "source": [
    "tvec_df = grid_search_models(models, \n",
    "                             pars, \n",
    "                             model_names, \n",
    "                             processings,\n",
    "                             'accuracy', \n",
    "                             X_train_t, \n",
    "                             y_train_t, \n",
    "                             X_test_t,\n",
    "                             y_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Processing</th>\n",
       "      <th>Train: Accuracy</th>\n",
       "      <th>Train: Precision</th>\n",
       "      <th>Train: Recall</th>\n",
       "      <th>Train: F1</th>\n",
       "      <th>Test: Accuracy</th>\n",
       "      <th>Test: Precision</th>\n",
       "      <th>Test: Recall</th>\n",
       "      <th>Test: F1</th>\n",
       "      <th>Cross-Val Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>KNN</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>LogisticRegression(C=5.541020330009492, class_...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model                                         Parameters  \\\n",
       "0                     KNN  KNeighborsClassifier(algorithm='auto', leaf_si...   \n",
       "0     Logistic Regression  LogisticRegression(C=5.541020330009492, class_...   \n",
       "0           Random Forest  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "0  Support Vector Machine  SVC(C=0.01, cache_size=200, class_weight=None,...   \n",
       "\n",
       "                            Processing Train: Accuracy Train: Precision  \\\n",
       "0  StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "0  StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "0  StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "0  StandardScaler(), TfidfVectorizer()           0.997            0.999   \n",
       "\n",
       "  Train: Recall Train: F1 Test: Accuracy Test: Precision Test: Recall  \\\n",
       "0         1.000     1.000          0.103           0.187        0.077   \n",
       "0         1.000     1.000          0.430           0.357        0.364   \n",
       "0         1.000     1.000          0.403           0.350        0.290   \n",
       "0         0.994     0.996          0.061           0.027        0.028   \n",
       "\n",
       "  Test: F1 Cross-Val Score  \n",
       "0    0.067           0.104  \n",
       "0    0.343           0.399  \n",
       "0    0.269           0.398  \n",
       "0    0.008           0.063  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec_df.to_csv('TfidfVectorizer_model_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Scale Model Testing with TfidfVectorizer and Engineered Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbor Classifier (KNN)**\n",
    "\n",
    "Pros\n",
    "* No training involved \n",
    "* Naturally handles multiclass classification and can learn complex decision boundaries\n",
    "* Uses feature similarity to predict the cluster that the new point will fall into\n",
    "* Does not assume any probability distributions on the training data. This can come in handy where the probability distribution is unknown.\n",
    "* Can quickly respond to changes in training data. KNN employs lazy learning, which generalizes during testing and this allows it to change during real-time use.\n",
    "\n",
    "Cons\n",
    "* The distance metric to choose it not obvious and difficult to justify in many cases\n",
    "* Performs poorly on high dimensional datasets \n",
    "* Sensitive to localized data - localized anomalies affect outcomes significantly, rather than for an algorithm that uses a generalized view of the data.\n",
    "* Computation time - Lazy learning requires that most of KNN's computation be done during testing, rather than during training. This can be an issue for large datasets.\n",
    "* Biased towards classes with more entries\n",
    "* Relies on a correlation between closeness and similarity. One workaround for this issue is dimension reduction, which reduces the number of working variable dimensions (but can lose variable trends in the process).\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "Pros\n",
    "* Low variance\n",
    "* Probability scores for target variable which can be investigated with P-R curves, ROC curves etc (only in binary target easily addressable)\n",
    "* Works well with diagonal (feature) decision boundaries\n",
    "* Overfitting can be addressed though regularization \n",
    "* Multi-collinearity is not really an issue and can be countered with L2 regularization to an extent\n",
    "* One-vesrus-one performs well on large dataset\n",
    "\n",
    "Cons\n",
    "* High bias\n",
    "* May overfit when provided with large numbers of features\n",
    "* Usually evaluates one-versus-all when predicting the classes\n",
    "* Doesn’t perform well when feature space is too large\n",
    "* Doesn’t handle large number of categorical features/variables well\n",
    "* Relies on transformations for non-linear features\n",
    "* Can only learn linear hypothesis functions so are less suitable to complex relationships between features and target\n",
    "\n",
    "**Decision Tree Classifier**\n",
    "\n",
    "Pros\n",
    "* easy to interpret visually when the trees only contain several levels\n",
    "* Can easily handle qualitative (categorical) features\n",
    "* Works well with decision boundaries parellel to the feature axis\n",
    "\n",
    "Cons\n",
    "* Prone to overfitting\n",
    "* Runs for a long time\n",
    "* Possible issues with diagonal decision boundaries\n",
    "* Can be very non-robust, meaning that small changes in the training dataset can lead to quite major differences in the hypothesis function that gets learned \n",
    "* Generally have worse performance than ensemble methods\n",
    "\n",
    "**Bagging Classifier - Decision Tree**\n",
    "\n",
    "Pros\n",
    "* Reduces variance in comparison to regular decision trees\n",
    "* Can provide variable importance measures\n",
    "* Can easily handle qualitative (categorical) features\n",
    "* Out of bag (OOB) estimates can be used for model validation\n",
    "\n",
    "Cons\n",
    "* Not as easy to visually interpret\n",
    "* Does not reduce variance if the features are correlated\n",
    "\n",
    "**Random Forest Classifier**\n",
    "\n",
    "Pros\n",
    "* Decorrelates trees (relative to bagged trees), which is especially useful when there is a lot of correlation\n",
    "* Reduced variance in comparison to regular decision tree\n",
    "* Has the ability to address class imbalance by using the balanced class weight flag\n",
    "* Scales to large datasets\n",
    "\n",
    "Cons\n",
    "* Not as easy to visually interpret\n",
    "* Long computation time when used in GridSearch\n",
    "* Tends to overfit on the training data but is claimed to not be susceptible to that\n",
    "\n",
    "**AdaBoost Classifier - Decision Tree**\n",
    "\n",
    "Pros\n",
    "* Can easily handle qualitative (categorical) features\n",
    "* Somewhat more interpretable than bagged trees/random forest as the user can define the size of each tree resulting in a collection of stumps (1 level) which can be viewed as an additive model\n",
    "\n",
    "Cons\n",
    "* Unlike bagging and random forests, can overfit if number of trees is too large\n",
    "\n",
    "**Multi-layer Perceptron Classifier**\n",
    "\n",
    "Pros\n",
    "* \n",
    "\n",
    "\n",
    "Cons\n",
    "* \n",
    "\n",
    "\n",
    "**Support Vector Classifier**\n",
    "\n",
    "Pros\n",
    "* Performs similarly to logistic regression when linear separation\n",
    "* Performs well with non-linear boundary depending on the kernel used\n",
    "* Handle high dimensional data well\n",
    "* Fairly robust against overfitting, especially in higher dimensional space\n",
    "\n",
    "Cons\n",
    "* Susceptible to overfitting/training issues depending on kernel\n",
    "* Do not scale well to large datasets (difficult to parallelize) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t, y_train_t, X_test_t, y_test_t = tvec_processing('lyrics_processed', X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t.drop('lyrics_processed', axis=1, inplace=True)\n",
    "X_test_t.drop('lyrics_processed', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t, X_test_t = StandardScaler_processing(X_train_t, X_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "log = LogisticRegression(max_iter=1000)\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "bag_tree = BaggingClassifier(base_estimator=dt, n_estimators=100)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "\n",
    "knn_params = {\n",
    "    'n_neighbors': range(1, 50),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "log1_params = {\n",
    "    'penalty': ['l1'],\n",
    "    'solver': ['liblinear'],\n",
    "    'multi_class': ['ovr'],\n",
    "    'C': np.logspace(-1, 1, 50)\n",
    "}\n",
    "\n",
    "log2_params = {\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs', 'sag'],\n",
    "    'multi_class': ['multinomial'],\n",
    "    'C': np.logspace(-1, 1, 50)\n",
    "}\n",
    "\n",
    "log_saga_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['saga'],\n",
    "    'multi_class': ['auto'],\n",
    "    'C': np.logspace(0, 1, 40)\n",
    "}\n",
    "\n",
    "dt_params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': list(range(1, 50)) + [None],\n",
    "    'max_features': [None, 20, 40, 60, 80, 100, 500, 1000, X_train_t.shape[1]],\n",
    "    'min_samples_split': [2, 10, 25, 50]\n",
    "}\n",
    "\n",
    "bag_tree_params = {\n",
    "    'max_samples': np.linspace(0.8, 1.0, 3),\n",
    "    'max_features': range(int(3 / 4. * X_train_t.shape[1]),\n",
    "                          X_train_t.shape[1] + 1)\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [500],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [40, 50, 60, 70] + [None],\n",
    "    'max_features': [None, 100, 150, 200]\n",
    "}\n",
    "\n",
    "ada_params = {\n",
    "    'base_estimator': [\n",
    "        DecisionTreeClassifier(max_depth=None),\n",
    "        DecisionTreeClassifier(max_depth=3),\n",
    "        DecisionTreeClassifier(max_depth=50)\n",
    "    ],\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate':\n",
    "    np.linspace(0.1, 1, 20)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'KNN', 'Logistic Regression L1', 'Logistic Regression L2',\n",
    "    'Logistic Regression, Saga Solver', 'Decision Tree',\n",
    "    'Bagging with Decision Tree', 'Random Forest',\n",
    "    'AdaBoost Classifier with Decision Tree'\n",
    "]\n",
    "\n",
    "models = [knn, log, log, log, dt, bag_tree, rf, ada]\n",
    "\n",
    "pars = [\n",
    "    knn_params, log1_params, log2_params, log_saga_params, dt_params,\n",
    "    bag_tree_params, rf_params, ada_params\n",
    "]\n",
    "\n",
    "processings = [\n",
    "    'StandardScaler(), TfidfVectorizer()',\n",
    "    'StandardScaler(), TfidfVectorizer()',\n",
    "    'StandardScaler(), TfidfVectorizer()',\n",
    "    'StandardScaler(), TfidfVectorizer()',\n",
    "    'StandardScaler(), TfidfVectorizer()',\n",
    "    'StandardScaler(), TfidfVectorizer()',\n",
    "    'StandardScaler(), TfidfVectorizer()',\n",
    "    'StandardScaler(), TfidfVectorizer()']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Gridsearch\n",
      "Running Model 1 / 8.\n",
      "Fitting 3 folds for each of 196 candidates, totalling 588 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:   32.8s\n",
      "[Parallel(n_jobs=-1)]: Done 588 out of 588 | elapsed:   55.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 2 / 8.\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 3 / 8.\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   50.6s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 33.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 4 / 8.\n",
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed: 54.7min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 94.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 5 / 8.\n",
      "Fitting 3 folds for each of 3600 candidates, totalling 10800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 412 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1630 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 3328 tasks      | elapsed:   24.9s\n",
      "[Parallel(n_jobs=-1)]: Done 5518 tasks      | elapsed:   41.0s\n",
      "[Parallel(n_jobs=-1)]: Done 7597 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 9178 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 10777 out of 10800 | elapsed:  1.8min remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 10800 out of 10800 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 6 / 8.\n",
      "Fitting 3 folds for each of 786 candidates, totalling 2358 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 989 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1434 tasks      | elapsed: 23.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1961 tasks      | elapsed: 32.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2358 out of 2358 | elapsed: 39.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 7 / 8.\n",
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   34.4s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 8 / 8.\n",
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "grid_df = grid_search_models(models, \n",
    "                             pars, \n",
    "                             model_names, \n",
    "                             processings,\n",
    "                             'accuracy', \n",
    "                             X_train_t, \n",
    "                             y_train_t, \n",
    "                             X_test_t,\n",
    "                             y_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Processing</th>\n",
       "      <th>Train: Accuracy</th>\n",
       "      <th>Train: Precision</th>\n",
       "      <th>Train: Recall</th>\n",
       "      <th>Train: F1</th>\n",
       "      <th>Test: Accuracy</th>\n",
       "      <th>Test: Precision</th>\n",
       "      <th>Test: Recall</th>\n",
       "      <th>Test: F1</th>\n",
       "      <th>Cross-Val Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>KNN</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression L1</td>\n",
       "      <td>LogisticRegression(C=2.442053094548651, class_...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression L2</td>\n",
       "      <td>LogisticRegression(C=0.21209508879201905, clas...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Logistic Regression, Saga Solver</td>\n",
       "      <td>LogisticRegression(C=7.443803013251689, class_...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Bagging with Decision Tree</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>AdaBoost Classifier with Decision Tree</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  \\\n",
       "0                                     KNN   \n",
       "1                  Logistic Regression L1   \n",
       "2                  Logistic Regression L2   \n",
       "3        Logistic Regression, Saga Solver   \n",
       "4                           Decision Tree   \n",
       "5              Bagging with Decision Tree   \n",
       "6                           Random Forest   \n",
       "7  AdaBoost Classifier with Decision Tree   \n",
       "\n",
       "                                          Parameters  \\\n",
       "0  KNeighborsClassifier(algorithm='auto', leaf_si...   \n",
       "1  LogisticRegression(C=2.442053094548651, class_...   \n",
       "2  LogisticRegression(C=0.21209508879201905, clas...   \n",
       "3  LogisticRegression(C=7.443803013251689, class_...   \n",
       "4  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "5  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "6  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "7  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "\n",
       "                            Processing Train: Accuracy Train: Precision  \\\n",
       "0  StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "1  StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "2  StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "3  StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "4  StandardScaler(), TfidfVectorizer()           0.414            0.366   \n",
       "5  StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "6  StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "7  StandardScaler(), TfidfVectorizer()           0.856            0.929   \n",
       "\n",
       "  Train: Recall Train: F1 Test: Accuracy Test: Precision Test: Recall  \\\n",
       "0         1.000     1.000          0.103           0.187        0.077   \n",
       "1         1.000     1.000          0.338           0.305        0.295   \n",
       "2         1.000     1.000          0.426           0.356        0.336   \n",
       "3         1.000     1.000          0.433           0.402        0.369   \n",
       "4         0.335     0.314          0.247           0.228        0.211   \n",
       "5         1.000     1.000          0.426           0.386        0.343   \n",
       "6         1.000     1.000          0.437           0.366        0.341   \n",
       "7         0.832     0.863          0.289           0.301        0.211   \n",
       "\n",
       "  Test: F1 Cross-Val Score  \n",
       "0    0.067           0.104  \n",
       "1    0.289           0.293  \n",
       "2    0.328           0.391  \n",
       "3    0.363           0.387  \n",
       "4    0.197           0.241  \n",
       "5    0.338           0.400  \n",
       "6    0.330           0.433  \n",
       "7    0.211           0.206  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_df.reset_index(drop=True, inplace=True)\n",
    "grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=25, p=2,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_df.Parameters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_knn = BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=5),\n",
    "                            n_estimators=50)\n",
    "\n",
    "bag_best_knn = BaggingClassifier(base_estimator=grid_df.Parameters[0],\n",
    "                                 n_estimators=50)\n",
    "\n",
    "bag_best_tree = BaggingClassifier(base_estimator=grid_df.Parameters[4],\n",
    "                                  n_estimators=100)\n",
    "\n",
    "MLP = MLPClassifier(max_iter=1000)\n",
    "\n",
    "\n",
    "\n",
    "bag_params = {\n",
    "    'max_samples': np.linspace(0.8, 1.0, 3),\n",
    "    'max_features': range(int(3 / 4. * X_train_t.shape[1]),\n",
    "                          X_train_t.shape[1] + 1)\n",
    "}\n",
    "\n",
    "neur_params = {\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [10**(-10), 10**(-5), 10**(-2)],\n",
    "    'hidden_layer_sizes': [(8, 8, 8), (10, 10, 10), (20, 10, 8, 8)],\n",
    "    'activation': ['identity', 'relu', 'logistic', 'tanh'],\n",
    "    'random_state': [42],\n",
    "    'batch_size': ['auto', 50, 500],\n",
    "    'early_stopping': [True],\n",
    "    'validation_fraction': [0.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names2 = [\n",
    "    'Bagging with KNN', 'Bagging with Best KNN', 'Bagging with Best Decision Tree',\n",
    "    'Multi-layer Perceptron Classifier'\n",
    "]\n",
    "\n",
    "models2 = [bag_knn, bag_best_knn, bag_best_tree, MLP]\n",
    "\n",
    "pars2 = [\n",
    "    bag_params, bag_params, bag_params, neur_params\n",
    "]\n",
    "\n",
    "processings2 = [\n",
    "    'StandardScaler(), TfidfVectorizer()',\n",
    "    'StandardScaler(), TfidfVectorizer()',\n",
    "    'StandardScaler(), TfidfVectorizer()',\n",
    "    'StandardScaler(), TfidfVectorizer()']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Gridsearch\n",
      "Running Model 1 / 4.\n",
      "Fitting 3 folds for each of 786 candidates, totalling 2358 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed: 28.5min\n",
      "[Parallel(n_jobs=-1)]: Done 989 tasks      | elapsed: 47.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1434 tasks      | elapsed: 73.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1961 tasks      | elapsed: 107.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2358 out of 2358 | elapsed: 130.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 2 / 4.\n",
      "Fitting 3 folds for each of 786 candidates, totalling 2358 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   59.7s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed: 32.1min\n",
      "[Parallel(n_jobs=-1)]: Done 989 tasks      | elapsed: 52.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1434 tasks      | elapsed: 79.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1961 tasks      | elapsed: 110.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2358 out of 2358 | elapsed: 134.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 3 / 4.\n",
      "Fitting 3 folds for each of 786 candidates, totalling 2358 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 989 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1434 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1961 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2358 out of 2358 | elapsed: 16.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 4 / 4.\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 324 out of 324 | elapsed:   20.6s finished\n"
     ]
    }
   ],
   "source": [
    "grid_df2 = grid_search_models(models2, \n",
    "                             pars2, \n",
    "                             model_names2, \n",
    "                             processings2,\n",
    "                             'accuracy', \n",
    "                             X_train_t, \n",
    "                             y_train_t, \n",
    "                             X_test_t,\n",
    "                             y_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Processing</th>\n",
       "      <th>Train: Accuracy</th>\n",
       "      <th>Train: Precision</th>\n",
       "      <th>Train: Recall</th>\n",
       "      <th>Train: F1</th>\n",
       "      <th>Test: Accuracy</th>\n",
       "      <th>Test: Precision</th>\n",
       "      <th>Test: Recall</th>\n",
       "      <th>Test: F1</th>\n",
       "      <th>Cross-Val Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Bagging with KNN</td>\n",
       "      <td>(KNeighborsClassifier(algorithm='auto', leaf_s...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Bagging with Best KNN</td>\n",
       "      <td>(KNeighborsClassifier(algorithm='auto', leaf_s...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Bagging with Best Decision Tree</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Multi-layer Perceptron Classifier</td>\n",
       "      <td>MLPClassifier(activation='identity', alpha=1e-...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Model  \\\n",
       "0                   Bagging with KNN   \n",
       "0              Bagging with Best KNN   \n",
       "0    Bagging with Best Decision Tree   \n",
       "0  Multi-layer Perceptron Classifier   \n",
       "\n",
       "                                          Parameters  \\\n",
       "0  (KNeighborsClassifier(algorithm='auto', leaf_s...   \n",
       "0  (KNeighborsClassifier(algorithm='auto', leaf_s...   \n",
       "0  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "0  MLPClassifier(activation='identity', alpha=1e-...   \n",
       "\n",
       "                            Processing Train: Accuracy Train: Precision  \\\n",
       "0  StandardScaler(), TfidfVectorizer()           0.528            0.927   \n",
       "0  StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "0  StandardScaler(), TfidfVectorizer()           0.700            0.753   \n",
       "0  StandardScaler(), TfidfVectorizer()           0.722            0.694   \n",
       "\n",
       "  Train: Recall Train: F1 Test: Accuracy Test: Precision Test: Recall  \\\n",
       "0         0.537     0.618          0.053           0.084        0.061   \n",
       "0         1.000     1.000          0.103           0.179        0.076   \n",
       "0         0.592     0.613          0.369           0.259        0.264   \n",
       "0         0.658     0.650          0.148           0.101        0.109   \n",
       "\n",
       "  Test: F1 Cross-Val Score  \n",
       "0    0.035           0.036  \n",
       "0    0.061           0.102  \n",
       "0    0.239           0.338  \n",
       "0    0.101           0.142  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Processing</th>\n",
       "      <th>Train: Accuracy</th>\n",
       "      <th>Train: Precision</th>\n",
       "      <th>Train: Recall</th>\n",
       "      <th>Train: F1</th>\n",
       "      <th>Test: Accuracy</th>\n",
       "      <th>Test: Precision</th>\n",
       "      <th>Test: Recall</th>\n",
       "      <th>Test: F1</th>\n",
       "      <th>Cross-Val Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>KNN</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression L1</td>\n",
       "      <td>LogisticRegression(C=2.442053094548651, class_...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression L2</td>\n",
       "      <td>LogisticRegression(C=0.21209508879201905, clas...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Logistic Regression, Saga Solver</td>\n",
       "      <td>LogisticRegression(C=7.443803013251689, class_...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Bagging with Decision Tree</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>AdaBoost Classifier with Decision Tree</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Bagging with KNN</td>\n",
       "      <td>(KNeighborsClassifier(algorithm='auto', leaf_s...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Bagging with Best KNN</td>\n",
       "      <td>(KNeighborsClassifier(algorithm='auto', leaf_s...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Bagging with Best Decision Tree</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Multi-layer Perceptron Classifier</td>\n",
       "      <td>MLPClassifier(activation='identity', alpha=1e-...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Model  \\\n",
       "0                                      KNN   \n",
       "1                   Logistic Regression L1   \n",
       "2                   Logistic Regression L2   \n",
       "3         Logistic Regression, Saga Solver   \n",
       "4                            Decision Tree   \n",
       "5               Bagging with Decision Tree   \n",
       "6                            Random Forest   \n",
       "7   AdaBoost Classifier with Decision Tree   \n",
       "8                         Bagging with KNN   \n",
       "9                    Bagging with Best KNN   \n",
       "10         Bagging with Best Decision Tree   \n",
       "11       Multi-layer Perceptron Classifier   \n",
       "\n",
       "                                           Parameters  \\\n",
       "0   KNeighborsClassifier(algorithm='auto', leaf_si...   \n",
       "1   LogisticRegression(C=2.442053094548651, class_...   \n",
       "2   LogisticRegression(C=0.21209508879201905, clas...   \n",
       "3   LogisticRegression(C=7.443803013251689, class_...   \n",
       "4   DecisionTreeClassifier(class_weight=None, crit...   \n",
       "5   (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "6   (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "7   (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "8   (KNeighborsClassifier(algorithm='auto', leaf_s...   \n",
       "9   (KNeighborsClassifier(algorithm='auto', leaf_s...   \n",
       "10  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "11  MLPClassifier(activation='identity', alpha=1e-...   \n",
       "\n",
       "                             Processing Train: Accuracy Train: Precision  \\\n",
       "0   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "1   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "2   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "3   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "4   StandardScaler(), TfidfVectorizer()           0.414            0.366   \n",
       "5   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "6   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "7   StandardScaler(), TfidfVectorizer()           0.856            0.929   \n",
       "8   StandardScaler(), TfidfVectorizer()           0.528            0.927   \n",
       "9   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "10  StandardScaler(), TfidfVectorizer()           0.700            0.753   \n",
       "11  StandardScaler(), TfidfVectorizer()           0.722            0.694   \n",
       "\n",
       "   Train: Recall Train: F1 Test: Accuracy Test: Precision Test: Recall  \\\n",
       "0          1.000     1.000          0.103           0.187        0.077   \n",
       "1          1.000     1.000          0.338           0.305        0.295   \n",
       "2          1.000     1.000          0.426           0.356        0.336   \n",
       "3          1.000     1.000          0.433           0.402        0.369   \n",
       "4          0.335     0.314          0.247           0.228        0.211   \n",
       "5          1.000     1.000          0.426           0.386        0.343   \n",
       "6          1.000     1.000          0.437           0.366        0.341   \n",
       "7          0.832     0.863          0.289           0.301        0.211   \n",
       "8          0.537     0.618          0.053           0.084        0.061   \n",
       "9          1.000     1.000          0.103           0.179        0.076   \n",
       "10         0.592     0.613          0.369           0.259        0.264   \n",
       "11         0.658     0.650          0.148           0.101        0.109   \n",
       "\n",
       "   Test: F1 Cross-Val Score  \n",
       "0     0.067           0.104  \n",
       "1     0.289           0.293  \n",
       "2     0.328           0.391  \n",
       "3     0.363           0.387  \n",
       "4     0.197           0.241  \n",
       "5     0.338           0.400  \n",
       "6     0.330           0.433  \n",
       "7     0.211           0.206  \n",
       "8     0.035           0.036  \n",
       "9     0.061           0.102  \n",
       "10    0.239           0.338  \n",
       "11    0.101           0.142  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_table = pd.concat([grid_df, grid_df2], axis=0, sort=False)\n",
    "test_table.reset_index(drop=True, inplace=True)\n",
    "test_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "\n",
    "linearSVC = LinearSVC()\n",
    "\n",
    "\n",
    "rbf_params = {\n",
    "    'C': np.linspace(0.01, 10, 20),\n",
    "    'gamma': np.linspace(0, 1, 10),\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "poly_params = {\n",
    "    'C': np.linspace(0.01, 10, 20),\n",
    "    'gamma': np.linspace(0.01, 1, 10),\n",
    "    'kernel': ['poly']\n",
    "}\n",
    "\n",
    "sigmoid_params = {\n",
    "    'C': np.linspace(0.01, 5, 20),\n",
    "    'gamma': np.linspace(0, 1, 10),\n",
    "    'kernel': ['sigmoid']\n",
    "}\n",
    "\n",
    "linearSVC_params = {\n",
    "    'penalty': ['l2'],\n",
    "    'loss': ['hinge', 'squared_hinge'],\n",
    "    'tol': [0.0001],\n",
    "    'C': np.linspace(0.001, 10, 30),\n",
    "    'multi_class': ['ovr']\n",
    "}\n",
    "\n",
    "linearSVC_params2 = {\n",
    "    'penalty': ['l2', 'l1'],\n",
    "    'loss': ['hinge', 'squared_hinge'],\n",
    "    'tol': [0.0001],\n",
    "    'C': np.linspace(0.001, 10, 30),\n",
    "    'multi_class': ['crammer_singer']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names3 = [\n",
    "    'Support Vector Classifier - rbf', 'Support Vector Classifier - poly',\n",
    "    'Support Vector Classifier - sigmoid', 'Linear Support Vector Classifier',\n",
    "    'Linear Support Vector Classifier'\n",
    "]\n",
    "\n",
    "models3 = [svc, svc, svc, linearSVC, linearSVC]\n",
    "\n",
    "pars3 = [\n",
    "    rbf_params, poly_params, sigmoid_params, linearSVC_params,\n",
    "    linearSVC_params2\n",
    "]\n",
    "\n",
    "processings3 = [\n",
    "    'StandardScaler(), TfidfVectorizer()',\n",
    "    'StandardScaler(), TfidfVectorizer()',\n",
    "    'StandardScaler(), TfidfVectorizer()',\n",
    "    'StandardScaler(), TfidfVectorizer()',\n",
    "    'StandardScaler(), TfidfVectorizer()'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Gridsearch\n",
      "Running Model 1 / 5.\n",
      "Fitting 3 folds for each of 200 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:   54.9s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 2 / 5.\n",
      "Fitting 3 folds for each of 200 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   23.7s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:   56.4s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 3 / 5.\n",
      "Fitting 3 folds for each of 200 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 4 / 5.\n",
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   39.1s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed: 10.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model 5 / 5.\n",
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:  5.8min finished\n"
     ]
    }
   ],
   "source": [
    "grid_df3 = grid_search_models(models3, \n",
    "                             pars3, \n",
    "                             model_names3, \n",
    "                             processings3,\n",
    "                             'accuracy', \n",
    "                             X_train_t, \n",
    "                             y_train_t, \n",
    "                             X_test_t,\n",
    "                             y_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Processing</th>\n",
       "      <th>Train: Accuracy</th>\n",
       "      <th>Train: Precision</th>\n",
       "      <th>Train: Recall</th>\n",
       "      <th>Train: F1</th>\n",
       "      <th>Test: Accuracy</th>\n",
       "      <th>Test: Precision</th>\n",
       "      <th>Test: Recall</th>\n",
       "      <th>Test: F1</th>\n",
       "      <th>Cross-Val Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>KNN</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression L1</td>\n",
       "      <td>LogisticRegression(C=2.442053094548651, class_...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression L2</td>\n",
       "      <td>LogisticRegression(C=0.21209508879201905, clas...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Logistic Regression, Saga Solver</td>\n",
       "      <td>LogisticRegression(C=7.443803013251689, class_...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Bagging with Decision Tree</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>AdaBoost Classifier with Decision Tree</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Bagging with KNN</td>\n",
       "      <td>(KNeighborsClassifier(algorithm='auto', leaf_s...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Bagging with Best KNN</td>\n",
       "      <td>(KNeighborsClassifier(algorithm='auto', leaf_s...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Bagging with Best Decision Tree</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Multi-layer Perceptron Classifier</td>\n",
       "      <td>MLPClassifier(activation='identity', alpha=1e-...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Support Vector Classifier - rbf</td>\n",
       "      <td>SVC(C=1.0615789473684212, cache_size=200, clas...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Support Vector Classifier - poly</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Support Vector Classifier - sigmoid</td>\n",
       "      <td>SVC(C=0.27263157894736845, cache_size=200, cla...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Linear Support Vector Classifier</td>\n",
       "      <td>LinearSVC(C=0.001, class_weight=None, dual=Tru...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Linear Support Vector Classifier</td>\n",
       "      <td>LinearSVC(C=0.001, class_weight=None, dual=Tru...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Model  \\\n",
       "0                                      KNN   \n",
       "1                   Logistic Regression L1   \n",
       "2                   Logistic Regression L2   \n",
       "3         Logistic Regression, Saga Solver   \n",
       "4                            Decision Tree   \n",
       "5               Bagging with Decision Tree   \n",
       "6                            Random Forest   \n",
       "7   AdaBoost Classifier with Decision Tree   \n",
       "8                         Bagging with KNN   \n",
       "9                    Bagging with Best KNN   \n",
       "10         Bagging with Best Decision Tree   \n",
       "11       Multi-layer Perceptron Classifier   \n",
       "12         Support Vector Classifier - rbf   \n",
       "13        Support Vector Classifier - poly   \n",
       "14     Support Vector Classifier - sigmoid   \n",
       "15        Linear Support Vector Classifier   \n",
       "16        Linear Support Vector Classifier   \n",
       "\n",
       "                                           Parameters  \\\n",
       "0   KNeighborsClassifier(algorithm='auto', leaf_si...   \n",
       "1   LogisticRegression(C=2.442053094548651, class_...   \n",
       "2   LogisticRegression(C=0.21209508879201905, clas...   \n",
       "3   LogisticRegression(C=7.443803013251689, class_...   \n",
       "4   DecisionTreeClassifier(class_weight=None, crit...   \n",
       "5   (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "6   (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "7   (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "8   (KNeighborsClassifier(algorithm='auto', leaf_s...   \n",
       "9   (KNeighborsClassifier(algorithm='auto', leaf_s...   \n",
       "10  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "11  MLPClassifier(activation='identity', alpha=1e-...   \n",
       "12  SVC(C=1.0615789473684212, cache_size=200, clas...   \n",
       "13  SVC(C=0.01, cache_size=200, class_weight=None,...   \n",
       "14  SVC(C=0.27263157894736845, cache_size=200, cla...   \n",
       "15  LinearSVC(C=0.001, class_weight=None, dual=Tru...   \n",
       "16  LinearSVC(C=0.001, class_weight=None, dual=Tru...   \n",
       "\n",
       "                             Processing Train: Accuracy Train: Precision  \\\n",
       "0   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "1   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "2   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "3   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "4   StandardScaler(), TfidfVectorizer()           0.414            0.366   \n",
       "5   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "6   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "7   StandardScaler(), TfidfVectorizer()           0.856            0.929   \n",
       "8   StandardScaler(), TfidfVectorizer()           0.528            0.927   \n",
       "9   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "10  StandardScaler(), TfidfVectorizer()           0.700            0.753   \n",
       "11  StandardScaler(), TfidfVectorizer()           0.722            0.694   \n",
       "12  StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "13  StandardScaler(), TfidfVectorizer()           0.997            0.999   \n",
       "14  StandardScaler(), TfidfVectorizer()           0.140            0.121   \n",
       "15  StandardScaler(), TfidfVectorizer()           0.986            0.993   \n",
       "16  StandardScaler(), TfidfVectorizer()           0.996            0.997   \n",
       "\n",
       "   Train: Recall Train: F1 Test: Accuracy Test: Precision Test: Recall  \\\n",
       "0          1.000     1.000          0.103           0.187        0.077   \n",
       "1          1.000     1.000          0.338           0.305        0.295   \n",
       "2          1.000     1.000          0.426           0.356        0.336   \n",
       "3          1.000     1.000          0.433           0.402        0.369   \n",
       "4          0.335     0.314          0.247           0.228        0.211   \n",
       "5          1.000     1.000          0.426           0.386        0.343   \n",
       "6          1.000     1.000          0.437           0.366        0.341   \n",
       "7          0.832     0.863          0.289           0.301        0.211   \n",
       "8          0.537     0.618          0.053           0.084        0.061   \n",
       "9          1.000     1.000          0.103           0.179        0.076   \n",
       "10         0.592     0.613          0.369           0.259        0.264   \n",
       "11         0.658     0.650          0.148           0.101        0.109   \n",
       "12         1.000     1.000          0.057           0.001        0.026   \n",
       "13         0.994     0.996          0.061           0.027        0.028   \n",
       "14         0.107     0.106          0.278           0.221        0.218   \n",
       "15         0.976     0.983          0.350           0.236        0.265   \n",
       "16         0.993     0.995          0.388           0.304        0.319   \n",
       "\n",
       "   Test: F1 Cross-Val Score  \n",
       "0     0.067           0.104  \n",
       "1     0.289           0.293  \n",
       "2     0.328           0.391  \n",
       "3     0.363           0.387  \n",
       "4     0.197           0.241  \n",
       "5     0.338           0.400  \n",
       "6     0.330           0.433  \n",
       "7     0.211           0.206  \n",
       "8     0.035           0.036  \n",
       "9     0.061           0.102  \n",
       "10    0.239           0.338  \n",
       "11    0.101           0.142  \n",
       "12    0.003           0.060  \n",
       "13    0.008           0.063  \n",
       "14    0.206           0.302  \n",
       "15    0.235           0.353  \n",
       "16    0.299           0.384  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_table = pd.concat([test_table, grid_df3], axis=0, sort=False)\n",
    "test_table.reset_index(drop=True, inplace=True)\n",
    "test_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Processing</th>\n",
       "      <th>Train: Accuracy</th>\n",
       "      <th>Train: Precision</th>\n",
       "      <th>Train: Recall</th>\n",
       "      <th>Train: F1</th>\n",
       "      <th>Test: Accuracy</th>\n",
       "      <th>Test: Precision</th>\n",
       "      <th>Test: Recall</th>\n",
       "      <th>Test: F1</th>\n",
       "      <th>Cross-Val Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Bagging with Decision Tree</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression L2</td>\n",
       "      <td>LogisticRegression(C=0.21209508879201905, clas...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Logistic Regression, Saga Solver</td>\n",
       "      <td>LogisticRegression(C=7.443803013251689, class_...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Linear Support Vector Classifier</td>\n",
       "      <td>LinearSVC(C=0.001, class_weight=None, dual=Tru...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Linear Support Vector Classifier</td>\n",
       "      <td>LinearSVC(C=0.001, class_weight=None, dual=Tru...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Bagging with Best Decision Tree</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Support Vector Classifier - sigmoid</td>\n",
       "      <td>SVC(C=0.27263157894736845, cache_size=200, cla...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression L1</td>\n",
       "      <td>LogisticRegression(C=2.442053094548651, class_...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>AdaBoost Classifier with Decision Tree</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Multi-layer Perceptron Classifier</td>\n",
       "      <td>MLPClassifier(activation='identity', alpha=1e-...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>KNN</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Bagging with Best KNN</td>\n",
       "      <td>(KNeighborsClassifier(algorithm='auto', leaf_s...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Support Vector Classifier - poly</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Support Vector Classifier - rbf</td>\n",
       "      <td>SVC(C=1.0615789473684212, cache_size=200, clas...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Bagging with KNN</td>\n",
       "      <td>(KNeighborsClassifier(algorithm='auto', leaf_s...</td>\n",
       "      <td>StandardScaler(), TfidfVectorizer()</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Model  \\\n",
       "6                            Random Forest   \n",
       "5               Bagging with Decision Tree   \n",
       "2                   Logistic Regression L2   \n",
       "3         Logistic Regression, Saga Solver   \n",
       "16        Linear Support Vector Classifier   \n",
       "15        Linear Support Vector Classifier   \n",
       "10         Bagging with Best Decision Tree   \n",
       "14     Support Vector Classifier - sigmoid   \n",
       "1                   Logistic Regression L1   \n",
       "4                            Decision Tree   \n",
       "7   AdaBoost Classifier with Decision Tree   \n",
       "11       Multi-layer Perceptron Classifier   \n",
       "0                                      KNN   \n",
       "9                    Bagging with Best KNN   \n",
       "13        Support Vector Classifier - poly   \n",
       "12         Support Vector Classifier - rbf   \n",
       "8                         Bagging with KNN   \n",
       "\n",
       "                                           Parameters  \\\n",
       "6   (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "5   (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "2   LogisticRegression(C=0.21209508879201905, clas...   \n",
       "3   LogisticRegression(C=7.443803013251689, class_...   \n",
       "16  LinearSVC(C=0.001, class_weight=None, dual=Tru...   \n",
       "15  LinearSVC(C=0.001, class_weight=None, dual=Tru...   \n",
       "10  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "14  SVC(C=0.27263157894736845, cache_size=200, cla...   \n",
       "1   LogisticRegression(C=2.442053094548651, class_...   \n",
       "4   DecisionTreeClassifier(class_weight=None, crit...   \n",
       "7   (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "11  MLPClassifier(activation='identity', alpha=1e-...   \n",
       "0   KNeighborsClassifier(algorithm='auto', leaf_si...   \n",
       "9   (KNeighborsClassifier(algorithm='auto', leaf_s...   \n",
       "13  SVC(C=0.01, cache_size=200, class_weight=None,...   \n",
       "12  SVC(C=1.0615789473684212, cache_size=200, clas...   \n",
       "8   (KNeighborsClassifier(algorithm='auto', leaf_s...   \n",
       "\n",
       "                             Processing Train: Accuracy Train: Precision  \\\n",
       "6   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "5   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "2   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "3   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "16  StandardScaler(), TfidfVectorizer()           0.996            0.997   \n",
       "15  StandardScaler(), TfidfVectorizer()           0.986            0.993   \n",
       "10  StandardScaler(), TfidfVectorizer()           0.700            0.753   \n",
       "14  StandardScaler(), TfidfVectorizer()           0.140            0.121   \n",
       "1   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "4   StandardScaler(), TfidfVectorizer()           0.414            0.366   \n",
       "7   StandardScaler(), TfidfVectorizer()           0.856            0.929   \n",
       "11  StandardScaler(), TfidfVectorizer()           0.722            0.694   \n",
       "0   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "9   StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "13  StandardScaler(), TfidfVectorizer()           0.997            0.999   \n",
       "12  StandardScaler(), TfidfVectorizer()           1.000            1.000   \n",
       "8   StandardScaler(), TfidfVectorizer()           0.528            0.927   \n",
       "\n",
       "   Train: Recall Train: F1 Test: Accuracy Test: Precision Test: Recall  \\\n",
       "6          1.000     1.000          0.437           0.366        0.341   \n",
       "5          1.000     1.000          0.426           0.386        0.343   \n",
       "2          1.000     1.000          0.426           0.356        0.336   \n",
       "3          1.000     1.000          0.433           0.402        0.369   \n",
       "16         0.993     0.995          0.388           0.304        0.319   \n",
       "15         0.976     0.983          0.350           0.236        0.265   \n",
       "10         0.592     0.613          0.369           0.259        0.264   \n",
       "14         0.107     0.106          0.278           0.221        0.218   \n",
       "1          1.000     1.000          0.338           0.305        0.295   \n",
       "4          0.335     0.314          0.247           0.228        0.211   \n",
       "7          0.832     0.863          0.289           0.301        0.211   \n",
       "11         0.658     0.650          0.148           0.101        0.109   \n",
       "0          1.000     1.000          0.103           0.187        0.077   \n",
       "9          1.000     1.000          0.103           0.179        0.076   \n",
       "13         0.994     0.996          0.061           0.027        0.028   \n",
       "12         1.000     1.000          0.057           0.001        0.026   \n",
       "8          0.537     0.618          0.053           0.084        0.061   \n",
       "\n",
       "   Test: F1 Cross-Val Score  \n",
       "6     0.330           0.433  \n",
       "5     0.338           0.400  \n",
       "2     0.328           0.391  \n",
       "3     0.363           0.387  \n",
       "16    0.299           0.384  \n",
       "15    0.235           0.353  \n",
       "10    0.239           0.338  \n",
       "14    0.206           0.302  \n",
       "1     0.289           0.293  \n",
       "4     0.197           0.241  \n",
       "7     0.211           0.206  \n",
       "11    0.101           0.142  \n",
       "0     0.067           0.104  \n",
       "9     0.061           0.102  \n",
       "13    0.008           0.063  \n",
       "12    0.003           0.060  \n",
       "8     0.035           0.036  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_table.sort_values('Cross-Val Score',ascending=False, inplace=True)\n",
    "test_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table.to_csv('Model_Test_sorted.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "420px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
